{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a word2vec model from DailyDialogue dataset, which contains 10,000+ text-based dialogues between two people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from python-dateutil>=2->pandas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: bz2file in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.37 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "install_packages = ['nltk','pandas','gensim','inflect']\n",
    "for i in install_packages:\n",
    "    pip.main(['install',i])\n",
    "import nltk,gensim,logging,pandas as pd, re, string, unicodedata, inflect\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "l = WordNetLemmatizer()\n",
    "s = PorterStemmer()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102979\n",
      "102980\n",
      "102979\n",
      "102978\n"
     ]
    }
   ],
   "source": [
    "# read the data from corpus\n",
    "dial_act = open(\"data/dialogues_act.txt\", 'r' , encoding = \"utf8\")\n",
    "dial_text = open(\"data/dialogues_text.txt\", 'r' , encoding = \"utf8\")\n",
    "dial_emo = open(\"data/dialogues_emotion.txt\", 'r' , encoding = \"utf8\")\n",
    "\n",
    "\n",
    "merged = []\n",
    "\n",
    "dial_act_read = dial_act.read().split(\" \")\n",
    "dial_act_read = dial_act_read[:-1]\n",
    "dial_act_read = list(map(int, dial_act_read))\n",
    "\n",
    "dial_emo_read = dial_emo.read().split(\" \")\n",
    "dial_emo_read = dial_emo_read[:-1]\n",
    "dial_emo_read = list(map(int,dial_emo_read))\n",
    "\n",
    "dial_text_read = dial_text.read().split(\"__eou__\")\n",
    "dial_text_read = dial_text_read[:-1]\n",
    "\n",
    "person = ['A' , 'B'] * (int(len(dial_emo_read)/2))\n",
    "\n",
    "print(len(dial_emo_read))\n",
    "print(len(dial_text_read))\n",
    "print(len(dial_act_read))\n",
    "\n",
    "merged = [person ,dial_act_read, dial_emo_read, dial_text_read]\n",
    "merged = list(zip(*merged))\n",
    "merged = pd.DataFrame(merged)\n",
    "merged.columns = ['Person' , 'Act' , 'Emotion' , 'Conversation']\n",
    "print(len(merged))\n",
    "dial_act.close()\n",
    "dial_text.close()\n",
    "dial_emo.close()\n",
    "\n",
    "dial_act_read = []\n",
    "dial_emo_read = []\n",
    "#dial_text_read = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Act</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>\\nSo Dick , how about getting some coffee for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee ? I don â€™ t honestly like that kind of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Come on , you can at least try a little , bes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Person  Act  Emotion                                       Conversation\n",
       "0      A    3        2                              The kitchen stinks . \n",
       "1      B    4        0                      I'll throw out the garbage . \n",
       "2      A    3        4  \\nSo Dick , how about getting some coffee for ...\n",
       "3      B    4        2   Coffee ? I don â€™ t honestly like that kind of...\n",
       "4      A    3        0   Come on , you can at least try a little , bes..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The kitchen stinks . ',\n",
       " \" I'll throw out the garbage . \",\n",
       " '\\nSo Dick , how about getting some coffee for tonight ? ',\n",
       " ' Coffee ? I don â€™ t honestly like that kind of stuff . ',\n",
       " ' Come on , you can at least try a little , besides your cigarette . ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dial_text_read[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    #words = stem_words(words)\n",
    "    #words = lemmatize_verbs(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'kitchen', 'stinks', '.', '']\n",
      "['dick', 'getting', 'coffee', 'tonight']\n"
     ]
    }
   ],
   "source": [
    "a = [dial_text_read[i].replace('\\n','').split(\" \") for i in range(10)]\n",
    "words = normalize(a[2])\n",
    "print(a[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = [normalize(dial_text_read[i].replace('\\n','').split(\" \")) for i in range(len(dial_text_read))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['kitchen', 'stinks'],\n",
       " ['ill', 'throw', 'garbage'],\n",
       " ['dick', 'getting', 'coffee', 'tonight'],\n",
       " ['coffee', 'honestly', 'like', 'kind', 'stuff'],\n",
       " ['come', 'least', 'try', 'little', 'besides', 'cigarette']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=10, size=200,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
