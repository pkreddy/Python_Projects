{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a word2vec model from DailyDialogue dataset, which contains 10,000+ text-based dialogues between two people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from python-dateutil>=2->pandas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: bz2file in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.37 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\prana\\anaconda3\\envs\\py35\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "install_packages = ['nltk','pandas','gensim','inflect']\n",
    "for i in install_packages:\n",
    "    pip.main(['install',i])\n",
    "import nltk,gensim,logging,pandas as pd, re, string, unicodedata, inflect\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "l = WordNetLemmatizer()\n",
    "s = PorterStemmer()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102979\n",
      "102980\n",
      "102979\n",
      "102978\n"
     ]
    }
   ],
   "source": [
    "# read the data from corpus\n",
    "dial_act = open(\"data/dialogues_act.txt\", 'r' , encoding = \"utf8\")\n",
    "dial_text = open(\"data/dialogues_text.txt\", 'r' , encoding = \"utf8\")\n",
    "dial_emo = open(\"data/dialogues_emotion.txt\", 'r' , encoding = \"utf8\")\n",
    "\n",
    "\n",
    "merged = []\n",
    "\n",
    "dial_act_read = dial_act.read().split(\" \")\n",
    "dial_act_read = dial_act_read[:-1]\n",
    "dial_act_read = list(map(int, dial_act_read))\n",
    "\n",
    "dial_emo_read = dial_emo.read().split(\" \")\n",
    "dial_emo_read = dial_emo_read[:-1]\n",
    "dial_emo_read = list(map(int,dial_emo_read))\n",
    "\n",
    "dial_text_read = dial_text.read().split(\"__eou__\")\n",
    "dial_text_read = dial_text_read[:-1]\n",
    "\n",
    "person = ['A' , 'B'] * (int(len(dial_emo_read)/2))\n",
    "\n",
    "print(len(dial_emo_read))\n",
    "print(len(dial_text_read))\n",
    "print(len(dial_act_read))\n",
    "\n",
    "merged = [person ,dial_act_read, dial_emo_read, dial_text_read]\n",
    "merged = list(zip(*merged))\n",
    "merged = pd.DataFrame(merged)\n",
    "merged.columns = ['Person' , 'Act' , 'Emotion' , 'Conversation']\n",
    "print(len(merged))\n",
    "dial_act.close()\n",
    "dial_text.close()\n",
    "dial_emo.close()\n",
    "\n",
    "dial_act_read = []\n",
    "dial_emo_read = []\n",
    "#dial_text_read = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Act</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>\\nSo Dick , how about getting some coffee for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee ? I don â€™ t honestly like that kind of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Come on , you can at least try a little , bes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Person  Act  Emotion                                       Conversation\n",
       "0      A    3        2                              The kitchen stinks . \n",
       "1      B    4        0                      I'll throw out the garbage . \n",
       "2      A    3        4  \\nSo Dick , how about getting some coffee for ...\n",
       "3      B    4        2   Coffee ? I don â€™ t honestly like that kind of...\n",
       "4      A    3        0   Come on , you can at least try a little , bes..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The kitchen stinks . ',\n",
       " \" I'll throw out the garbage . \",\n",
       " '\\nSo Dick , how about getting some coffee for tonight ? ',\n",
       " ' Coffee ? I don â€™ t honestly like that kind of stuff . ',\n",
       " ' Come on , you can at least try a little , besides your cigarette . ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dial_text_read[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    #words = stem_words(words)\n",
    "    #words = lemmatize_verbs(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'kitchen', 'stinks', '.', '']\n",
      "['dick', 'getting', 'coffee', 'tonight']\n"
     ]
    }
   ],
   "source": [
    "a = [dial_text_read[i].replace('\\n','').split(\" \") for i in range(10)]\n",
    "words = normalize(a[2])\n",
    "print(a[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = [normalize(dial_text_read[i].replace('\\n','').split(\" \")) for i in range(len(dial_text_read))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['kitchen', 'stinks'],\n",
       " ['ill', 'throw', 'garbage'],\n",
       " ['dick', 'getting', 'coffee', 'tonight'],\n",
       " ['coffee', 'honestly', 'like', 'kind', 'stuff'],\n",
       " ['come', 'least', 'try', 'little', 'besides', 'cigarette']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 45802 words, keeping 5720 word types\n",
      "PROGRESS: at sentence #20000, processed 100671 words, keeping 9453 word types\n",
      "PROGRESS: at sentence #30000, processed 149317 words, keeping 10969 word types\n",
      "PROGRESS: at sentence #40000, processed 199719 words, keeping 13024 word types\n",
      "PROGRESS: at sentence #50000, processed 251412 words, keeping 15073 word types\n",
      "PROGRESS: at sentence #60000, processed 306476 words, keeping 16765 word types\n",
      "PROGRESS: at sentence #70000, processed 368421 words, keeping 18317 word types\n",
      "PROGRESS: at sentence #80000, processed 417574 words, keeping 19617 word types\n",
      "PROGRESS: at sentence #90000, processed 478587 words, keeping 21099 word types\n",
      "PROGRESS: at sentence #100000, processed 542635 words, keeping 22359 word types\n",
      "collected 22772 word types from a corpus of 562115 raw words and 102980 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=10 retains 4844 unique words (21% of original 22772, drops 17928)\n",
      "min_count=10 leaves 518427 word corpus (92% of original 562115, drops 43688)\n",
      "deleting the raw counts dictionary of 22772 items\n",
      "sample=0.001 downsamples 52 most-common words\n",
      "downsampling leaves estimated 461254 word corpus (89.0% of prior 518427)\n",
      "estimated required memory for 4844 words and 200 dimensions: 10172400 bytes\n",
      "resetting layer weights\n",
      "training model with 3 workers on 4844 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 562115 raw words (461541 effective words) took 0.6s, 718511 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 562115 raw words (461325 effective words) took 0.5s, 840795 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 562115 raw words (461034 effective words) took 0.7s, 676953 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 562115 raw words (461104 effective words) took 0.6s, 751945 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 562115 raw words (461354 effective words) took 0.7s, 677454 effective words/s\n",
      "training on a 2810575 raw words (2306358 effective words) took 3.3s, 692065 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(text_clean, min_count=10, size=200, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving Word2Vec object under model, separately None\n",
      "not storing attribute vectors_norm\n",
      "not storing attribute cum_table\n",
      "saved model\n"
     ]
    }
   ],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Word2VecKeyedVectors object from model\n",
      "loading wv recursively from model.wv.* with mmap=None\n",
      "setting ignored attribute vectors_norm to None\n",
      "loading vocabulary recursively from model.vocabulary.* with mmap=None\n",
      "loading trainables recursively from model.trainables.* with mmap=None\n",
      "setting ignored attribute cum_table to None\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "model =  gensim.models.KeyedVectors.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2VecVocab at 0x14c82e996a0>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['kitchen'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0758995 ,  0.09349342, -0.07778248,  0.157159  , -0.15246977,\n",
       "       -0.03411049,  0.14633189,  0.2388728 , -0.23951001,  0.07900506], dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['kitchen'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971223799649\n",
      "0.521276783487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\prana\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('king', 'queen'))\n",
    "print(model.similarity('love', 'dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the context words you can get the probability of center words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('company', 0.00071256171), ('room', 0.00061875832), ('already', 0.00046515366), ('china', 0.00044574862), ('business', 0.00043729166), ('also', 0.00043230929), ('many', 0.00042863266), ('ive', 0.0004098186), ('course', 0.00040292248), ('service', 0.00040252384)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_output_word(['emergency', 'beacon', 'received']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x14c82a86e10>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
